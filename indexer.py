import os
from bs4 import BeautifulSoup
import json
import re
import porter
from urllib.parse import urlparse
import hashlib

def create_dir():
    try:
        os.mkdir("finalIndex")
        os.mkdir("map")
        os.mkdir("reduce")
    except:
        print("directory already exist!")


#this function get all of the paths of the files we are required to index. Those#directories and files are stored at DEV/. 
def get_files():
    file_list = []
    l = os.listdir("DEV")
    for i in l:
        if i != ".DS_Store":
            k = os.listdir("DEV/" + i) 
            for f in k:
                file_list.append("DEV/" + i + "/" + f)
    return file_list
#This function seperate the entire 50000+ pages into 2500 pages per batch.
def get_batch(fl):
    toReturn = []
    batch = []
    counter_1 = len(fl)
    counter_2 = 0
    while counter_1 > 0:
        counter_1 -= 1
        batch.append(fl[counter_1])
        counter_2 += 1
        if counter_2 == 2500 or counter_1 == 0:
            toReturn.append(batch)
            batch = []
            counter_2 = 0
    return toReturn

#This function seperate all of the files generated by the map_f function into 10
#files per batch.

def get_aggre_batch(fl):    
    toReturn = []
    batch = []
    counter_1 = len(fl)
    counter_2 = 0
    while counter_1 > 0:
        counter_1 -= 1
        batch.append(fl[counter_1])
        counter_2 += 1
        if counter_2 == 10 or counter_1 == 0:
            toReturn.append(batch)
            batch = []
            counter_2 = 0
    return toReturn


#This function load the content of the json file into a dictionary
def get_content(f_name):
    f = open(f_name,"r")
    toReturn = json.load(f)
    f.close()
    return toReturn


#This function check if the url is a valid url to prevent index large file.
def is_valid_url(url):
    try:
        parsed = urlparse(url)
        if parsed.scheme not in set(["http", "https"]):
            return False
        elif re.match(
                r".*\.(ff|css|js|bmp|gif|jpe?g|ico"
                + r"|png|tiff?|mid|mp2|mp3|mp4"
                + r"|wav|avi|mov|mpeg|ram|m4v|mkv|ogg|ogv|pdf"
                + r"|ps|eps|tex|ppt|pptx|doc|docx|xls|xlsx|names"
                + r"|data|dat|exe|bz2|tar|msi|bin|7z|psd|dmg|iso"
                + r"|epub|dll|cnf|tgz|sha1|rar|doc|sql|lua|h|c|sr"
                + r"|thmx|mso|arff|rtf|jar|md|csv|@uci.edu|@ics.uci.edu"
                + r"|rm|smil|wmv|swf|wma|zip|rar|gz|txt|jpg|java|py|ppsx|scm)$",
                parsed.path.lower()):
            return False 
        elif re.match(
                r".*\.(ff|css|js|bmp|gif|jpe?g|ico"
                + r"|png|tiff?|mid|mp2|mp3|mp4"
                + r"|wav|avi|mov|mpeg|ram|m4v|mkv|ogg|ogv|pdf"
                + r"|ps|eps|tex|ppt|pptx|doc|docx|xls|xlsx|names"
                + r"|data|dat|exe|bz2|tar|msi|bin|7z|psd|dmg|iso"
                + r"|epub|dll|cnf|tgz|sha1|rar|doc|sql|lua|h|c|sr"
                + r"|thmx|mso|arff|rtf|jar|md|csv|@uci.edu|@ics.uci.edu"
                + r"|rm|smil|wmv|swf|wma|zip|rar|gz|txt|jpg|java|py|ppsx|scm)$",
                parsed.query.lower()):
            return False
        else:
            return True
    except TypeError:
        print("TypeError for ", parsed)
        raise

        

def get_positions(text,soup,tag):
    toReturn = []
    for t in soup.find_all(tag):
        if t.string != None:
            temp = text.find(t.string.lower())
            toReturn.append((temp, temp+len(t.string)))
    return toReturn




#This function loads files we are about to index and process the word by using 
#porter2 stemmer algorithm I implemented in porter.py. Indexes are dumped every #2500 pages. The extension of the dump is Json and the basic format of the dump is "word" : [post, post, post], post is a list in [doc_id, [positions],term_frequency]. At the end of this function, the map between doc_id and url will be write to another file.

def map_f():
    hashed_page = set()
    fl = get_files()
    total_f = len(fl)
    fl = get_batch(fl)
    page_indexed = 0
    doc_dict = {}
    inverted = {}
    counter = 0

    word_set = set()
    for c in range(0,len(fl)):
        for f in fl[c]:
            print(str(counter) + " : " + f)
            file_stats = os.stat(f)
            print(file_stats)
            b = file_stats.st_size / (1024 * 1024)
            print(f'File Size in MegaBytes is {b}')
            if b > 0.2:
                print("file too large")
                counter += 1
                total_f -= 1
                continue
            content = get_content(f)
            if not is_valid_url(content["url"]):
                counter += 1
                total_f -= 1
                continue
            soup = BeautifulSoup(content["content"],"html.parser")    
            for script in soup(["style"]):
                script.extract()
            text = soup.get_text().lower()
            myP = r"[a-z0-9]{2,23}"
            target_list = re.findall(myP,text)
            hashv = hash(tuple(sorted(list(set(target_list)))))
            if hashv in hashed_page:
                counter += 1
                total_f -= 1
                continue
            hashed_page.add(hashv)
            doc_dict[str(counter)] = [content["url"],[],[],[],[],[],[]]     #[url, title positions, header 1, header 2, header 3,strong/bold postions,anchor]
            doc_dict[str(counter)][1].extend(get_positions(text,soup,'title'))
            doc_dict[str(counter)][2].extend(get_positions(text,soup,'h1'))
            doc_dict[str(counter)][3].extend(get_positions(text,soup,'h2'))
            doc_dict[str(counter)][4].extend(get_positions(text,soup,'h3'))
            doc_dict[str(counter)][5].extend(get_positions(text,soup,'b'))
            doc_dict[str(counter)][5].extend(get_positions(text,soup,'strong'))
            doc_dict[str(counter)][6].extend(get_positions(text,soup,'a'))
            count = len(target_list)
            for i in range(0,count):
                t = porter.porter(target_list[i])
                word_set.add(t)
                if t not in inverted:
                    inverted[t] = {}
                    inverted[t][str(counter)] = [[i],count]
                elif str(counter) not in inverted[t]:
                    inverted[t][str(counter)] = [[i],count]
                else:
                    inverted[t][str(counter)][0].append(i)
            counter += 1
        keys = inverted.keys()
        #print(keys) 
        for key in keys:
        #v -> [[positions],count]
            new_v = []
            for post in inverted[key].items():
                doc_id, v = post
                new = (doc_id, v[0], len(v[0]))
                new_v.append(new)
            inverted[key] = new_v
        with open(f"map/batch_{c}.json", "w") as f:
            json.dump(inverted, f)
        inverted.clear()
    with open(f"finalIndex/url_id_match.json", "w") as f:
        json.dump(doc_dict, f)
    print(total_f)
    return word_set

#This function aggregate the resulted files of map_f function into smaller number of files.
def aggre():
    file_list = []
    k = os.listdir("map")
    for f in k:
        file_list.append("map/" + f)
    batch_list = get_aggre_batch(file_list)
    counter = 0
    for batch in batch_list:
        inverted = {}
        for f in batch:
            inde = get_content(f)
            keys = inde.keys()
            for key in keys:
                if key in inverted:
                    inverted[key].extend(inde[key])
                else:
                    inverted[key] = inde[key] 
        with open(f"reduce/agg_{counter}.json", "w") as f:
            json.dump(inverted, f)
        counter += 1


#Updated: This function save all words that started with the same alphabetic character in the same txt file and create a index file that record their starting index in order to use file.seek. Words thai
# start with other characters are stored in the other.txt.
def finalize(word_set):
    initChar = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']
    file_list = []
    k = os.listdir("reduce")
    EI = {}
    position = {}
    for f in k:
        file_list.append("reduce/" + f)
    counter = 0
    for c in initChar:
        inverted = {}
        for f in file_list: 
            inde = get_content(f)
            keys = inde.keys()
            for key in keys:
                if c == key[0] and key in inverted:
                    inverted[key].extend(inde[key])
                elif c  == key[0]:
                    inverted[key] = inde[key]
        for key in inverted.keys():
            inverted[key].sort(key=lambda x : int(x[2]),reverse=True)
        with open(f"finalIndex/{c}.txt", "w") as f:
            for k,v in inverted.items():
                position[k] = f.tell()
                for i in range(len(v)):
                    f.write(v[i][0] + ".")
                    f.write(str(v[i][1]) + ".")
                    f.write(str(v[i][2]))
                    if i != len(v) - 1:
                        f.write("|")
                    else:
                        f.write("\n")
    for f in file_list:
        inde = get_content(f)
        keys = inde.keys()
        for key in keys:
            if key[0] not in initChar and key in EI:
                EI[key].extend(inde[key])
            elif key[0] not in initChar:
                EI[key] = inde[key]
    for key in EI.keys():
        EI[key].sort(key=lambda x : int(x[2]),reverse=True)
    with open(f"finalIndex/other.txt", "w") as f:
        for k,v in EI.items():
            position[k] = f.tell()
            for i in range(len(v)):
                f.write(v[i][0] + ".")
                f.write(str(v[i][1]) + ".")
                f.write(str(v[i][2]))
                if i != len(v) - 1:
                    f.write("|")
                else:
                    f.write("\n")
    with open("finalIndex/position.json","w") as f:
        json.dump(position, f)
    print(f"num of words dump in the end: {len(position.keys())}")



if __name__ == "__main__":
    create_dir()
    word_set = map_f()
    aggre()
    finalize(word_set)
    print(len(word_set))
